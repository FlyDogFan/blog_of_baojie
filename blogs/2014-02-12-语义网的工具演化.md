语义网的工具演化
---
    
> Categories: Uncategorized, 语义网  
> Time: 2014-02-12  
> Original url: <http://baojie.org/blog/2014/02/12/semantic-tool-evolution/>
    
汇总了一些这个题目上的微博，组织了一下

参 2013-01-24《瘦语义网的几点想法》

### 工具，工具，工具

不有个笑话吗，乘客：“去卡内基音乐厅最短的路是什么？”出租车司机：“练习，练习，练习！”

对于语义网也是一样。到语义网最短的路是什么？练习，练习，练习。

练习就需要工具。

语义网这个领域不缺理论，就缺系统和工具。现有工具分为三类1 学校搞出来的代表是Protege，做NSF项目还行，对开发人员不友好。2 小公司面向政府和行业用户搞出来，代表是TopBraid Composer & Pellet，也对开发人员不友好。3 大公司总结搞出来内部用的，代表是Trinity & Graphd，一般人看都看不到

工具系统的竞争，是一个复杂的系统工程，绝不是一个标准化组织能组织和规划的。而工具的产生和演进，又是和用户与工程师的需求，理解能力和使用习惯密切相关的。基于w3c规范的工具系统，往往有太浓厚的学术性，不太贴合普通web工程师的需求。其实广义上讲，语义网已经是现实了: 大家不都用json吗?

切割磁力线可以发电。但从这个原理出发到电动机，还有铜丝，绝缘，线圈，转子，电刷等等一系列的“小问题”。从电动机到电网，还有一系列的发电，变电，输电问题，和各种法律，社会，经济上下游工程问题。“语义数据可以促进数据交换”这是个原理。从原理到语义网的实践，还有无数的工程问题

还用发电机做比喻，当前这个阶段，不是要去发明变压器，远程输电线这些“大数据”问题（规模问题），而是要搞铜丝，线圈，转子这些工具，这些“小数据”问题。先有了电，再有电网。先有了语义，再有了语义网。当前阶段，就应该好好搞工具，各种结构化数据产生消费的小工具。赶大数据的时髦并不健康

工具的改进，有对人不友好的问题，有对机器不友好的问题。我觉得这几年的当务之急是对人不友好的问题。云啊，可伸缩性啊，这些问题可以放一放，先把结构化数据生成的代价降下来再说。自然语言理解，数据库，用户交互这些都有用。个人认为核心问题还是用户交互，NLP是起一个辅助的作用

工具的进化，说到底也一个社区的进化。是一群人建立了工具。所以从现有工具演化语义网的工具，也就是促进现有的程序员社区演化，用他们熟悉的语言，提供思路给他们解决他们头痛已久的问题。JSON, JS, Python, Lucene等等，这些社区是希望所在

开发知识管理系统需要“人肉基础设施”： 共识、权威、信任、通信、沟通。语义网界常把语言和工具看作最重要的基础设施： OWL, SPARQL, Triple Store，大规模推理等。这并不错。问题是，如果机器基础设施不能很好支持人肉基础设施，那机器的作用也不可能被发挥出来。这往往是失败的语义网项目的共同点

http://t.cn/zYsf10g 基于在企业的工作，我列过一个不完全清单，在处理语义问题时，涉及的实用工具有哪些。工具的发展，不可能无中生有，大部分还是要在现有的工程基础上演化出来。知识的提取，存储，交换，展示，都应该尽可能贴近普通工程师的需要

Lean Semantic Web的两个基本点 1） 在需求获取层面上，面向目标用户开发语言和工具; 从种子用户出发而不是开始就试图满足所有人的需要; 强调做快速build, measure, learn 2）在技术层面上，消弱早期语义网对精确性、完备性、一致性、模型论语义等要求，优先满足可实现性、可用性和分布性(BASE)

### 阶段任务

语义问题是个大问题，web 3.0, web 4.0甚至web 5.0都面临的问题。Google Knowledge Graph，相对语义网当年的预期，可能实现了一个零头不到，可工程上已经是很难了，也是建立在过去十多年千百人的工作基础上的。语义问题工程化，就该抛掉一下子建立通用方案的幻想，认真从现有系统上做演化，积跬步

Nova Spivack 2007年的观点，贩卖一下：到2020年web3.0要达到的是数据互联问题，用现在的话说是knowledge graph(KG)要普及。到2030年web 4.0要达到的是数据智能互联和流动问题，也就是KG上的查询和一些推理。 Web5太远了，不需要设想。

每个阶段有自身核心任务。2007年开始推linked data，这个很好。先有政府数据，然后是大公司知识图谱，也都很好。没有Linked Data培养的这一批人才就没有知识图谱。下一步就应该是扩散了。现在各大公司的KG团队都有人出来创业，过几年工具生态系统，各种商业模式应该就丰富了。2020年KG普及目标有望实现

语义网的云计算的话，当前阶段还是应该侧重最基础的数据整合和分布式查询，例如mongodb, neo4j, elastic search这样的工具。或者是一些图的分布式分析工具，如spark, graphchi, Pregel。慢慢往上走，过几年上一个台阶。分布式RDF查询都尚早，更不用说分布式RDF推理、OWL推理。

### 要演进

语义网讲的数据互联，并不是说非要搞RDF HTTP URI。像webdis这样的工具，可以轻松地把一个应用的数据结构暴露给另一个应用。这样演进，充分利用现有的技术堆栈，比从头来建立一个RDF的技术栈要可行得多

工具和标准的演化，相互促进。过去十年，是W3C先制定标准，然后期待工具跟进。这个策略不怎么成功。似乎应该反过来，就让大家重新发明轮子，百花齐放，进化。多总结，少设计。要设计也不要搞工作组来设计，用Github就好

W3C的文档一个问题就是用程序员不熟悉的语言，一般人读不懂，包括那些Primer。Semantic Web for the Working Ontologist这本书卖得火，也就是浅显易懂，贴近实际。工具的安装，文档，这些细节往往决定了成败，不仅仅是功能

### 语言

以前说过很多RDF的问题了，这里再补充几条

现在觉得语义网以前大多数工具都是Java写的是工具不好用的一个重要原因。这不仅是一个语言的原因，更是思路和哲学的原因。语义网开发更适合用Python而非Java，更非C++

工具语言上，一定要放开RDF，要用JSON。连XM,YAML都不行，必须是JSON。XML,YAML相比RDF已经很简单了，工具支持已经很多了，都不行。RDF在普通程序员那里是一点机会都没有的。Web 3.0就是基于JSON的数据互联，句号。

举个例子W3C有一个HTTP Vocabulary in RDF http://t.cn/8FpuVzA。把HTTP头数据设计了schema，写成RDF。有意义吗？我觉得没有，不信去看附录里A practical Example有多繁琐。现实中程序员是怎么做的？像样一点的http包如requests，头信息直接就json了，非常简单好用。

你非要搞个schema，那头里面有非标准字段怎么办？domain range错了怎么办？http:这个url前缀又不是一般人能添加的。用JSON就没有问题，自由得很。而且往文档数据库里一丢，查找很方便

rdf要求主语和谓语是uri(不考虑无名节点的话)。这个要求很过分1)大多数实体和关系在创建之初是没有uri的。是先有房子后有门牌号而不是反过来 2) 可寻址的uri的增长远远慢于实体和关系的增长 3) 大多数发布者不会建网页，生成一个http uri认知上挑战很大，技术上不会。JSON就没这些问题

大多数用户连起个标签名字都困难，更比用说给关系和实体起个url名字了。事实上，他们连关系和实体的区别都搞不清楚。这里讲的可不是终端用户，而是开发者用户。原来在RPI找了40个计算机本科学生做实验，训练了一天以后大半还是不能正确区分关系和实体。普通用户更可想而知了

为什么会有人认为仅仅做个d2rq，rdf就能解决关系数据库不能解决的问题呢? 这种对rdf的迷信，恰恰是语义网迄今普及不利的原因。技术之间的竞争，往往不仅是能力的竞争，而是整个工具系统之间的竞争。语义网的rdf阵营，在工具系统上的劣势，不是几年能弥补上的

其实根本不需要RDF，把Python里面一个子集拿出来就够了。其他的很多tooling问题都迎刃而解，很多RDF里的伪问题（比如bNode问题）都不存在了。Turtle用Python定义一页纸就可以了，然后所有的Python包（比如NetworkX）都是语义网的工具

没有模型论语义是很关键的。不要说一般程序员，就是语义网里非逻辑方向的资深研究者，也很少有人能理解模型论语义。rdf和owl里面一些奇怪的设定都是基于模型论语义。图的操作则很直观，而且可以实现大多数常见推理模式，无需严格语义

说句实话，也许RDF有一个模型论语义从一开始就是错的。图数据库和Gremlin没有语义也工作的很好吗！或者，有一个operational semantics就够了，就是些图的操作吗。很多蛋疼的设计，如字符串不能做主语，都能解了。

OWL中的开放世界假设其实是基于这样一个假设，就是用户对完整全面的信息感兴趣，所以他才需要关心世界所有地方对某个资源是怎么描述的。这现在看显然不属于语义网的最小功能产品mvp。图数据库反其道而行之，任务信息的查找一般都是局部，递进的。个人觉得这更靠谱

### 数据库的问题

对机器效率的改善，我觉得当前应该充分发掘现有数据库的潜力，redis, elasticsearch/solr, mongodb, MySQL, neo4j等。不必搞什么通用方案，就针对特定问题（如社交关系）搞特定方案，开源就行。也不用太担心数据量的问题，大多数问题都不大

图数据库比语义数据库好的四个原因 1 构造更简单，对字符串更友好，避免过早优化 2 对大规模并行支持好，有现成的解决方案 3 工具系统集成好，json数据交换 4 本身支持sparql甚至推理，所以相对语义数据库并没失去什么

Neo4j之类，大企业还是用不了，水平扩展性不足。语义搜索的核心其实是图数据库，但是现有的产品都不好用，各大公司只好自己发明轮子。虽然我认为图数据库是NoSQL中最有前途的向，但也是最不成熟的方向。Titan也许有希望，可能会有更好的出来。

redis体现了一个趋势，就是把数据结构数据库化。集合，哈系表，堆栈，列表，简单对象，这些结构现在都可以用数据库的方式来管理。这么做的好处 1)也就是语义网的核心思想之一，可以把智能从代码里转移一些到数据里，便于数据与应用的分离 2) 基础优化服务化，减轻程序员的一些麻烦，如内存管理

目前的语义数据库，在DB-Engines Rank http://t.cn/zlSGITm 上最好的Virtuoso，55名。Jena 77，Sesame 84 Algebraix 119，AllegroGraph 121。我觉得有希望改造对语义网友好的，PostgreSQL 4名，MongoDB 6，Solr 12，Redis 13，Elasticsearch 21, Neo4j 22 MarkLogic 39

### 智能数据

Memect关心的是Smart Data智能数据问题。相对于大数据的Volume，Velocity, Variety，智能数据关心的是其他三个V：Veracity 数据的质量高于数量 Versatile 数据的多能性：应用与数据的分离，数据语义的自描述 Value（也是最重要的）数据的价值：面向应用，以最少代价，最少数据满足最重要的需要

Memect致力于开放数据的互联。不过我们关心的重点，不是数据的发布或格式，而是数据的消费，数据对人的友好性。不仅是通过可视化工具等更好的表示，而是在数据产生，转移，转化，汇总，分离，最后产生价值的全周期中，都是以对人（包括用户和开发者）友好的形式来组织。

也即，智能数据是面向数据价值的精益创业lean startup

传统语义网研究从客户，价值主张，战略伙伴到渠道建设，大多是围绕着企业用户展开的。智能数据的去企业化，并不是说不能为企业用户所用，而是突破传统企业开发中的组织浪费，漫长的反馈周期，和单一化的价值取向。

大数据是数据计划经济。智能数据是数据市场经济

数据川流不息。如果不沉淀就还是数据。只有沉淀才创造价值。传统的大数据是服务器端的沉淀，是摩擦力大，模式有限的沉淀。智能数据是更lean的沉淀，适合更复杂多样的需求，去中心化，低摩擦力。

大数据风潮中滋生了一种观点，认为数据量大重要，模型不重要，因果性也不重要。这是数据生态中的r策略。智能数据或者大知识则是k策略，注重数据的质量而非数量。人和鲸鱼之类是k策略，老鼠和昆虫都是r策略，各自在不同的生态位成功。

一言以蔽之，智能数据是语义网的去企业化，摆脱三十年来知识管理领域的若干弊端: 对机器友好而不是对人友好，过长周期的价值反馈，瀑布式的开发流程，工具系统对工业程序员不友好。也就是说，smart data就是用精益lean思想改造过的语义网，也就是lean semantic web

知识也是一种数据。什么样的数据? 首先是浓缩的数据，是大量一般数据的总结和精华。其次是需要反复重用的数据，重用中产生的价值超过生成知识的代价。最后，知识很少单独发生作用，它的价值一般发生于和普通数据交互，使普通数据增值。总结: 知识是催化剂，知识是浓缩铀，知识是打折卡

\#认知剩余# 减小智能数据的发布门槛。智能数据smart data是语义网的又一营销马甲。用url来定位数据有助可发现性，但是提高了数据发布门槛。schema的采样是一种权威。最经济的方式应该是自由的，不需要权威许可和转移的。不需要定位的唯一性，只需要大致唯一性，最终唯一性或上下文唯一性

\#认知剩余# 人工智能最好的应用方式不是代替人的智能，而是更好的连接人的智能。机器学习其实把数据中隐含的人的智能汇总的方法。新的智能数据smart data的方法将更直接的促进解决复杂的智能问题     
    