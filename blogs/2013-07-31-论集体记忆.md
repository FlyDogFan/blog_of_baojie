论集体记忆
---
    
> Categories: Uncategorized, 语义网  
> Time: 2013-07-31  
> Original url: <http://baojie.org/blog/2013/07/31/on-collective-memory/>
    
原文 http://baojie.org/blog/2013/07/31/on-collective-memory/1 原则1.1 以人为本1.2 Web 3.0基本属性1.2.1 Smart Data1.2.2 Distributed1.2.3 Refined and Personalized1.2.4 Open1.3 个人记忆1.4 集体记忆2 技术手段2.1 RDF的地位2.2 知识库的构造与增长2.3 HCI的重要性2.4 知识索引原则以人为本知识管理的第一要务不是知识提取(例如机器学习，自然语言处理)，不是知识表现，而是知识重用。重用包括时间上的，也即记忆，和空间上的，也即分享。总体的，知识重用是研究集体记忆的科学和方法论。知识重用的核心并非对机器友好的知识表现，而是对人友好的知识表现。传统KR领域往往忽视了人机交互以及人际交互中知识表现的特性。前者是hci问题，后者是tbl讲的social machine问题。解决了这两个，知识管理中最瓶颈的地方就好办了。再说一遍，解决AI问题的核心是人而不是机器,有多少人工就有多少智能知识提取和知识表现是为知识重用服务的。人才是知识工程的核心。几乎所有有用的知识都是人产生的。机器学习是一种间接知识重用方法。这种方法风靡十年后，可能会被更直接的方法取代软件构架的核心是人。语义网作为最野心勃勃的软件构架之一，它早期的设计也映射了制造它的组织本身: 政府研究机构，学校，标准化组织。这些组织都不擅长对应复杂多变的需求，所以设计的软件构架难以实现规模化。后期的语义网能够规模化，需要更分布式，更灵活的组织来推动其设计。TBL说，Web从来不仅是技术的发明，更多的是一种社会的创造。无论是HTTP还是PageRank，无论是Wiki还是Facebook，人的因素是主导因素。新一代的Web的技术，必然的依然要以人的需要、长处、局限、价值为出发点。一个Web的语言，使用它的应该是人而不是机器。OWL的问题大家都看到了，RDF又如何呢？这次在#semtech# ，和人工智能领域一个大型项目负责人谈了很久。检讨项目的经验教训，核心的问题之一在于如何利用好人的智能。智能的采集和交换，是比智能的提取(不管是知识工程那种人工方法还是机器学习那种”自动”方法)更核心的问题。说到底是人的智能。知识提取大行于social web。基于知识表现的semantic web没能起飞。我觉得smart web并不是semantic web，它是基于知识重用的今天读了几章The Art of Readable Code。推荐。觉得可以推广一下，到The Art of Readable Knowledge。传统上总是把知识看作机器表达，知识表现是用一种机器可理解的语言来表述人类知识。这种看法是片面的——知识表现语言是一种计算机程序，和所有的程序一样，其对人的可读性及其重要。在这个标准上，无论是最近的RDF，OWL, RIF， 还是更传统的KIF，都不是很合适人类阅读的语言。写作这样的知识库很困难，理解这样的知识库更困难。”readable knowledge”是lean semantic web里很重要的一个原则。Web 3.0基本属性流程的分布性，数据的智能性，资源的可发现性，应用的未知性，是Web 3.0区别与Web2.0的几个核心优势，也是新的平台协议和软件必须具有的特征Future of the World WIde Web by Jim Hendler 里面再次提到social machine的概念。Web 3.0是人和机器合作的网络——目前为止的Web 2.0主要是人和人合作的网络。人做难的那部分，机器做容易的那部分。http://www.slideshare.net/jahendler/future-of-the-world-wide-web-india我怀疑Web 3.0上的搜索引擎（如果还这么叫的话），不会是”crawl”出来的。从Yahoo到Google，从G到Facebook，都不是简单的外推和加法，而是依托一种新的信息组织与发现的模式。关键还是要产生价值。Web 3.0会带来什么？我想就是不仅是社交、文档、电子商务，而是人们生活的方方面面，那些现在不上Facebook的人，他们的生活也全面Web化——其实这个空间远远没有饱和。在技术上，就是从社交Web进化到(大)数据Web。现实中市场经济不只广告，Web上也会一样Smart DataNova Spivack有个说法，Web 3.0是Data Web, 就是结构化数据时代，Web 4.0才是Intelligent Web，把语义网算是完全实现了。我觉得他说的有道理。怎么从用户数据到结构化数据，怎么构造这个社会工程，大概也是Web3.0的重要特征DistributedWeb设计的核心原则在于分布性。不仅是指物理媒介的分布性，如网络和服务器，更重要的是流程的分布性，去中心性。资源在进入网络时尽可能减少阻力，让每个人都按他自己熟悉的方法进行。HTTP和URI的设计体现了这种哲学，RDF等系列语言都违背了这个哲学。新的平台协议则要进一步支持流程的分布性我觉得数据和应用的分离是关键。从数据奴隶制度走向资本主义市场经济//@ShangguanRPI: native或者web都只关乎技术，未来的mobile app更重要的是形态，跟着用户走的形态。Refined and Personalized一个词总结Web 3.0：做减法。这是逆Web 2.0的趋势而动。Web在未来10年会渗透到70亿人中现在还不活跃于Web2.0那80%的人，延续Web2.0 的思路是不成的。怎么做减法，各种技术，包括新型用户界面，各种新型数据录用、采集方式，数据理解，用户理解，语义分析，高质量（结构化）数据，push技术，等等。在Web 2.0时代，谁能浪费用户最多的时间，谁就能挣钱。在Web 3.0时代，谁能节约用户用户最多的时间，谁就能挣钱 //@西瓜大丸子汤: 广告是做加法，推给用户更多的信息。在小屏幕上，大概要做减法，减少给用户的信息Open通信！Web是一种通信的工具。关起门来发展的，不是Web的技术。TBL近年一直严厉批评Facebook，因为它（及类似的其他公司）试图建立数据的壁垒，阻碍数据的流动。早年我不理解这种批评，最近两年来，慢慢理解并开始宗教般地坚信TBL的论断。Facebook的短板就是它的优势本身。一定会有开放数据模式打败FBThe Problem of Walled Gardens http://www.w3.org/2005/Incubator/socialweb/XGR-socialweb-20101206/#Problem Web之前很久就有超文本了？为什么到Web才爆发？因为URI和HTTP使任何文本可以和任何文本互通，而不仅仅是在单一的系统中。同样，互通现在封闭的社交网络，可能激发出十倍百倍的信息的力量，甚至不仅是Web的一种力量——正如Web不仅是Internet看完了《Weaving the Web》这本书。感想太多了，虽然是12年前的书，对今天的指导意义依然很大。特别是对理解social web和semantic web的不同命运——尽管TBL在书里规划了后者而不是前者。开放，交流，合作。Web里，技术只是一小部分，社会模式的变迁才是最根本的。长江后浪推前浪，后人可以做得更好A Standards-based, Open and Privacy-aware Social Web http://www.w3.org/2005/Incubator/socialweb/XGR-socialweb-20101206/ 2010年12月的W3C Social Web Incubator Group（ http://www.w3.org/2005/Incubator/socialweb/ ）报告：如何打破Facebook之类的数据壁垒。以后的社交网络，应该和电子邮件一样，任何人可以和任何网络上的任何人通信The creation of a decentralized and federated Social Web, as part of Web architecture, is a revolutionary opportunity to provide both increased social data portability and enhanced end-user privacy. http://www.w3.org/2005/Incubator/socialweb/XGR-socialweb-20101206/ 如果说下一个十年Web最大的机会是什么，我觉得是开放数据WebOracle的总部大楼象征了一个过去的时代: 数据放在一个一个data silo里。下一个web 3.0的大企业总部该什么样来体现其哲学？魔方怎么样？在开放数据Web上，用户不需要多个帐号。分布式的统一的身份认证将使任何人可以可任何人建立联系。那种个人数据被一个服务商锁定、过分甚至恶意使用的现象将得到限制。Facebook， G+等是社交网络目前的门户。正如Web 1.0的内容不会锁定在几个门户里，Social Web的内容也不会永远锁定在几个门户里聪明的社交网络服务提供者，应该主动适应数据互通、数据的使用可追责的趋势。Web的发展反复地证明，凡是试图控制数据、让所有的数据只在自己系统里玩的，最后都会在开放内容的冲击下没落。开放Web和语义Web天然是一孪生体。语义Web其实并不仅是关于“语义”（本体，推理等），更主要是让数据以最小摩擦在各Web主体间流动。不论是机器可查询、自动化、还是数据聚合，都是减少数据摩擦力。XML为什么还不够？XML Schema的形成本身就是摩擦力。开放Web并不需要事事基于约定的schema或者ontology开放数据的Web，并不仅是为了用户的方便。Web开始只是为了文档互联和查找的方便，而其经济和社会后果远远超越于此。新一代的Web，使数据流动起来，将带来全新的经济模式：数据本身成为最大量的商品。这种经济的规模，不会是社交网络的几百亿美元可比的。货币如果不能流通也就不是货币了。数据也一样。个人记忆Web3.0的一个重要特点将是辅助用户降低工作记忆(working memory)负担。首先是后端数据结构的革命，各种on-demand, just-in-time的数据建模、检索方式，特别是图和语义这两种工作。其次是前端HCI的革命，以Google Glass为代表的可穿戴计算机，帮助并行思维和辅助短期记忆。两前后端相辅相成，相互促进。文字是一次革命，线性记录思想。Web是一次革命，网络连接思想。下一次革命，把每个人的大脑本身Web化，非线性记录思想。智力越来越多的取决于大脑作为索引快速获取extended mind的能力。Faceted Browser走出了一小步。Google Glass和其他的可穿戴计算机将走出一大步。帮助用户发现甚至他们自己都不知道怎么表达的需要，启发用户，允许用户将工作记忆从recall变成recognition，这会是极其惊人的革命//@西瓜大丸子汤: Web3.0的一个重要特点将是辅助用户降低工作记忆(working memory)负担人工记忆不是要取代人脑的记忆，也无需模仿人脑的记忆。只要做好索引的职责就够了知识的形成有不同的粒度。传统RDF关注的粒度很小，形成起来很困难。大一点的粒度，相对就比较容易了，并不需要太多的AI。认知心理学里有两个结论，一个是识别(recognition)优于回忆(recall)，一个是环境信息有助记忆根据米勒法则，人的working memory capacity（工作记忆容量）大约是7个。也就是，要不产生压力，每天早上未读信件加上微博（或随便其他什么消息）应该少于7个。海量的信息流其实对心理健康和工作效率是有害的。难的是低门槛的，持续转化短期记忆为外在存储，还要加上对这些数据的语义理解与检索。Google Glass之类的平台出来以后，可能带来比Web更大的革命。 //@unk89v: 外在存储不就可以么，加上快捷的搜寻机制就能达到回忆效果了。回忆也是这回事情吧要是能把短期记忆的哪怕很小一部分持续转化为长期记忆，对人类认知，人类知识积累的贡献可能是仅次于文字发明，大于Websiri的角色本质上就是一级缓存。但是现在自然语言理解和结构化数据都不够成熟。语音界面更不合适。工程上更可行的一级缓存其实有很大一个市场，有极大的颠覆潜力。     神经元是我的寄存器，搜索引擎是二级缓存，整个数字世界是硬盘。现在缺一个一级缓存。长期记忆的形成一般有三种途径。一，反复强化，死记硬背，构成最基础的记忆。如语言和数学的一些基础知识。二，关联，实现启发式的记忆查找。网络搜索其实就是利用搜索引擎做启发式工具。三，推理，做基于符号的记忆内容重建。长期记忆的关键是发生关联。evernote这一点很差。知识就是关系。要建立关系，就要先建立知识的单元和名字。微博在这个意义上，是一个伟大的知识工程：找到一个合适的知识单元，然后让这些单元组合起来，并能在已知关系上产生新的关系集体记忆知识管理的核心任务是辅助个人的长期记忆。以此为基础辅助长期记忆的分享。知识表现要以人为本，而非以机器为本集体记忆的形成，不是靠consensus，而是靠shared understanding。任何两个人的世界观是不一样的，新的知识融入某人的记忆，不是靠强迫的知识结构转移，而是在试验错中新知识找到在个人知识体系中的连接点。知识管理的关键也就在减少共享认识过程中的摩擦。比如事先定义的ontology就最好避免集体记忆需要的知识表现方法，为了适应普通用户，需要容忍不精确性。但不精确性并不是通过概率的方法。知识表现的目的并不是推理，而是促进知识在时间和空间两个维度的低摩擦流动。摩擦的主要阻力在用户界面而不是知识提取记忆和分享这两种需求是联系又矛盾的。分享并不仅是分享链接，记忆并不仅是抓取内容碎片。记忆是世界观数字化的过程。分享是世界观的一部分融合的过程。这两个过程都需要知识提取和知识表现来辅助。各自有系列软件工具。但是还没有一个对非黑客友好的统一方法在合作开发中，文档的作用不是做为团队的长期集体记忆，更不是作为团队的通讯手段。文档的索引，包含人脑中的一级索引和软件中的二级索引，甚至比产生文档本身更重要。会议的目的之一就是产生这些索引。有结构的东西不容易众包，因为结构是一种世界观。freebase能成功就不是众包。众包的conceptnet十几年了也不太成功//@潘越_: 传统的知识工程（比如建立企业词汇表）其实很难采用分包的方式来做，因为标准难以一致 。 //@陈利人:必须要协同工作，共享，众包。传统的基于本体的知识表现无法处理个人化的多种多样的世界观。集体记忆如果不依赖暴力或极为耗时的协调，必须采用web式的分布式设计，减少世界观摩擦，让所有人按他们舒服的方式组织 //@我:记忆是世界观数字化的过程。分享是世界观的一部分融合的过程集体记忆市场还处于胚胎状态，但是这个市场可能是继门户，搜索引擎，社交网络后，下一个引领Web数据革命的市场，这个市场的容量也会远远大于现有的搜索引擎+社交网络的总和。这个市场的主要技术手段是一系列知识重用的方法，各种人工智能的方法综合运用帮助人与人的知识交换，但核心是人而不是机器智能技术手段RDF的地位要回归rdf作为知识交换语言的本位。当年从kif, daml一路走下来，本意是做知识交换，后来却做成了数据库。所谓的瘦语义网lean semantic web，强调用更简单的数据处理语言，更对人友好的数据交互语言。rdf对这两个任务都不太胜任有语义是有很多好处，不过也可以考虑更贴近通用开发语言的语义，如operational semantics//@潘越_:同意rdf回歸作為知識交換語言。在這一用途下，有一個嚴格的語義框架，比如模型論語義，還是非常必要的。//@西瓜大丸子汤:要回归rdf作为知识交换语言的本位RDF不会成为新一代知识基础设施的处理语言和建模语言。使用triple数据模式并不意味着就是RDF，正如使用树形数据结构并不意味着使用XML。事实上我们也看到某些宣传使用RDF的系统，本质上是图数据库或仅用RDF做交互语言。正如Json简化了xml，新一代的知识交互语言和处理语言也会取代RDFRSS是一种知识传播协议。那里面那个R本来就是RDF。两者都是网景公司的贡献。后来RDF渐渐远离了人与人知识交换，专注于机器与机器的交换。RSS十年没有进步，其实孕育着极大的机会。RDF之长与RSS之长再次汇聚。市场颠覆的力量大概就在其中了。多年以后谷歌一定会后悔。知识库的构造与增长知识库按每1-2年一个数量级的速度在增长。现在还只是基础知识库，未来几年可能出现个人知识库，增长速度可能更快//@潘越_: 知识库增长速度快于数据库怎么讲？ //@西瓜大丸子汤:知识库的增长速度目前快于数据的增长速度，今后几年会更快。在5年左右时间，知识质量的优势将超过数据数量的优势。回复@Gary南京:考察知识库的增长，从cyc方法到现在的knowledge graph方法，知识库从10^6到10^10只花了6-7年。差不多每过一年到两年最大的知识库就要大一个数量级。由于基数小，知识增长的绝对量并不大，但趋势迅猛。随着更人性化的知识重用方法的普及，知识的积累可能更快重要的不是数量的增长，而是数据本身质量的变化。从相对简单的以人为枢纽的数据网络，到以多样性实体和信息单元为节点的多样知识网络。在终端上，如何利用知识做减法而不是利用数据做加法。如果眼睛盯着十亿用户，连一亿都嫌少，那很容易陷入创新者的窘境，被主流市场绑架了。新型的知识库不是传统的KR模式，也不是Freebase模式。在新模式下，构造成本是极低的。“认知剩余”的汇总//@the王晗: 那构建知识库的成本将如何呢？//@西瓜大丸子汤: 简言之，数据这一块成本相对上升，模型和知识库这一块成本相对下降，回复@SiDT: 同意前面一半。信息结构化还是要靠人工。其实人在产生信息的过程中本来是有很多结构的。好的知识重用方法能减少结构的浪费就很好了，效果可能会大大好于基于机器学习的方法。 //@SiDT:回复@西瓜大丸子汤:就是说，现阶段还是应该在知识抽取？把网上已有的大量信息结构化为知识？？不过根据我的不完全统计，语义数据在最近5年的发展，大体上每年涨一个数量级，远超内存的增长。估计三到五年后，语义数据的分析和使用将面临很大的大数据挑战。这都是高质量数据，不是打酱油数据，意义很大 //@西瓜大丸子汤: 到2012年1月，搜索引擎可见的语义网的规模有多大？17b数据，放内存里也就几THCI的重要性搜索依赖于长期记忆(long-term memory)。新型用户界面利用短期记忆（short-term memory）降低长期记忆的要求。如何产生这些短期记忆，需要用户分析和内容分析。这就是为什么语义技术与新型HCI是密不可分的表面上注意力是稀缺资源，内在的认知原因是人的工作记忆（working memory）是有限的。Google帮助人降低了长期记忆的负担。下一个能帮助人降低工作记忆负担的公司，是有潜力发展为下一个Google的。会是Evernote吗?知道用什么关键词一般依赖于长期记忆 //@Alisoncastle:为啥说搜索是长期记忆啊？我一搜完，看了就忘了人类作为智慧生命，每一次手指或鼠标点击都是知识创造。这是势。解放这个势能会有排山倒海的力量各种AI和HCI的方法的综合运用，体现在在各个层面降低数据的摩擦力，并进一步降低知识的摩擦力。底层各种索引技术，界面技术，语义技术，都到了大规模工业化的前夜新的hci和social machine的方法着眼点是促进人的智能的互联，把智能数字化的速度提高几个数量级。传统依赖专家的工作将分布到整个人群来做，包括那些不重度使用社交网络的人群可穿戴计算机时代对数据的需求将不是查找，而是发现。准备那些不可预知性，数据要为不可预知的应用存在，应用要处理不可预知的数据。用户将把长期记忆和短期记忆逐步转移到网络上。这些记忆将构成一个十万亿美元级别的市场。语义技术天生就是为这样的应用和市场准备的。搜索是人类历史上非常新的事物。搜索相对浏览，需要更高的背景知识，记忆力和智能。进一步降低这些门槛可能激发一个更大的市场。这需要的更多的是hci的进步。相对系统的其他模块(机器学习，自然语言处理，结构化数据库)，hci现在是木桶上最短的那块板没有鼠标就没有Web。没有触摸屏就没有移动Web。没有Siri（及它代表的一大票现在还没有出来的新用户界面）就没有Semantic WebSemantic Wiki对Semantic Web本身就是一个HCI的贡献，而不是知识结构、推理等传统方向。其中对重要的一个变化是，在wiki上，程序和数据的变化在界面上立即可见，而建界面本身的工作只需要极小的投入（尽管不一定好看）。设想一下，从头从HTML, Web Form, SPARQL, Triple Store搭起要多少功夫？David Karger: 相比ftp, web只做了”很小”的工作流变化: 用url直接关联文档，用点击立即访问链接，用浏览器让用户停留在一个应用中。Web没有创造新的东西，但使旧的东西更简单。语义网能做到同样的事吗?知识索引先阶段能规模化的语义关系很浅。仅仅是要支持同义和上下位概念关系，对搜索引擎索引的资源要求都要大一个数量级传统推理机设计，可以向数据库系统和信息检索系统学习很多。知识的索引远远复杂于数据库和全文索引，但也有很多既有的工作可以借鉴。但是在传统知识工程或逻辑的教学中，这方面的介绍基本没有。索引的代价很大，价值也很大。传统逻辑界研究往往忽视了为了规模化哪怕是很简单的推理，在实际工业部署中所需要的代价。简单引入多个单词之间的关系都要十倍几十倍的索引。现在工业化的，都是基于内存模式，简化为图上的可达性查询的一些推理，数据量也就是在T这个量级。要支持更复杂的推理，知识索引方法需要一些革命性的突破。知识索引的原理并不复杂，就好像pagerank本身数学公式也不复杂。但是要把多项式时间甚至指数时间算法通过索引转化为常数时间，索引本身又不能太大，对工程有极大挑战。但是解决这类问题的基础已经具备，市场需求和工程投入有可能进入良性循环索引技术这一块，是和机器学习，数据库和信息检索联系比较紧密的领域。这方向可能会出现重大突破，解决传统知识表现的性能瓶颈。各种图数据库和一些新一代检索引擎已经提供了初步可用的工具。各种对知识，而不仅是数据或文档进行检索的方法不断涌现。该领域的创业也在过去一年中不断出现rdf最大的挑战之一是数据库索引。triple建模使索引复杂，冗余不得不极大。在cpu占有，磁盘空间，内存占有上，主流triple store如virtuoso都数倍甚至十倍于关系数据库或者mongodb之类。综合成本，性能，功能，在正式产品中rdf数据库极少被使用也就不奇怪了RDF数据库由于三元组的无组织性(organization, context)，索引结构不免复杂和冗余。同样规模的数据，triple store和图数据库比，磁盘空间消耗常大10倍，相应的I/O和网络消耗都大，性能上不能满足需求也就可以理解了     
    